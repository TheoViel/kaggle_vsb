{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "63108ed7f840eba7d84f1fb5e6774a66316c1b4c"
   },
   "source": [
    "# Features Engineering Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pywt\n",
    "import scipy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numba import jit\n",
    "from numpy.fft import *\n",
    "from scipy.signal import *\n",
    "from math import log, floor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "from statsmodels.robust import mad\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.model_selection import *\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.signal import periodogram, welch\n",
    "from tsfresh.feature_extraction.feature_calculators import *\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../input/data/train.parquet'\n",
    "TEST_PATH = '../input/data/test.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = '../input/features/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "481f1062cadb64df8776fc2b92fe8202c8e34d8a"
   },
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46fedcc452ef62f57183b2ef65ef1fc61bd689e9"
   },
   "source": [
    "### High Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "54dbec902f949dfb4ad6463c23035ba0dfd414f9"
   },
   "outputs": [],
   "source": [
    "def butterworth(x, low_cutoff=10000, sample_rate=40000000):   \n",
    "    nyquist = 0.5 * sample_rate # nyquist frequency is half the sample rate https://en.wikipedia.org/wiki/Nyquist_frequency\n",
    "    norm_low_cutoff = low_cutoff / nyquist\n",
    "    sos = butter(10, Wn=[norm_low_cutoff], btype='highpass', output='sos')\n",
    "    return sosfilt(sos, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd0e21485bafc3d70f834514a96941f11dd6e49d"
   },
   "source": [
    "### Wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "ff3682b72c38c5a30d1da270e8751fbce156aa5a"
   },
   "outputs": [],
   "source": [
    "def madev(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4a72a8969dbc9794f1a817f635df74c9f2c4fe53"
   },
   "outputs": [],
   "source": [
    "def wavelet_lp(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * madev(coeff[-level])\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = (pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafc88144037074973094f5d116f381f14498322"
   },
   "source": [
    "### Peak Features\n",
    "- Features regarding the peaks in the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "2fc3f4588b1fd5c9d7df0b9cca9cfce21bd9cded"
   },
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "prominence = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7d745f6b4c761b3065aeed83963b469dacaeb7e8"
   },
   "outputs": [],
   "source": [
    "def get_peaks(x, threshold=0, prominence=0, max_width=5):\n",
    "    peaks, properties = find_peaks(x, threshold=threshold, prominence=prominence, width=(0, max_width), height=[-300, 300])\n",
    "    return peaks, properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00bc1cefe76b21d677e4b8b72fc45e27f865cb4c"
   },
   "source": [
    "- *peak_heights* :Height of the peaks\n",
    "- *left_thresholds* : Peaks vertical distance to its left neighbour\n",
    "- *right_thresholds* : Peaks vertical distance to its right neighbour\n",
    "- *prominences* : Measures how much peaks stand out from the surrounding baseline of the signal and is defined as the vertical distance between peaks and their lowest contour line.\n",
    "- *left_bases*, *right_bases* : The peaks’ left and right base indices. The higher base of each pair is a peak’s lowest contour line.\n",
    "- *widths* : Width of the peaks\n",
    "- *width_heights* : The height of the contour lines at which the widths were evaluated.\n",
    "- *left_ips* , *right_ips* : Interpolated positions of left and right intersection points of a horizontal line at the respective evaluation height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "b86c0dadbc75dfbf805f27c87141e22eeffaa271"
   },
   "outputs": [],
   "source": [
    "useful_props = ['peak_heights', 'thresholds', 'prominences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "54b1d2323bfc7ecdff0752918edef481f441caa3"
   },
   "outputs": [],
   "source": [
    "def peak_features(signal, useful_properties=useful_props, percentiles=[1, 5, 95, 99]):\n",
    "    features = []\n",
    "    peaks, properties = get_peaks(signal, threshold=threshold, prominence=prominence)\n",
    "    properties['thresholds'] = properties['left_thresholds'] + properties['right_thresholds'] / 2\n",
    "    \n",
    "    if len(peaks) > 1:\n",
    "        # Features on coordinates\n",
    "        features.append(len(peaks) / len(signal)) # density\n",
    "        gaps = np.abs(peaks[1:] - peaks[:-1])\n",
    "        features.append(np.mean(gaps))\n",
    "        features += list(np.percentile(gaps, percentiles))\n",
    "\n",
    "        # Features on properties\n",
    "        for prop in useful_properties:\n",
    "            features += [np.mean(properties[prop])]\n",
    "            features += list(np.percentile(properties[prop], percentiles))\n",
    "    else:\n",
    "        features = np.zeros((2 + len(percentiles) + (1 + len(percentiles)) * len(useful_props)))\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafc88144037074973094f5d116f381f14498322"
   },
   "source": [
    "### Distribution Features\n",
    "- Features on basic statistics (mean, std, percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_features(signal, percentiles=[0, 0.1, 1, 5, 95, 99, 99.9, 100]):\n",
    "    features = []\n",
    "    mean = np.mean(signal)\n",
    "    std = np.std(signal)\n",
    "    features += [mean, std, mean + std, mean - std]\n",
    "    perc = np.percentile(signal, percentiles)\n",
    "    features += [perc[-1] - perc[0]]\n",
    "    features += list(perc)\n",
    "    features += list(perc - np.mean(signal))\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafc88144037074973094f5d116f381f14498322"
   },
   "source": [
    "### Ts-fresh Features\n",
    "- Features using the ts-fresh package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsfresh_features(x):\n",
    "    features = []\n",
    "    features.append(abs_energy(x))\n",
    "    \n",
    "    max_bins = 100\n",
    "    features.append(binned_entropy(x, max_bins))\n",
    "    \n",
    "    normalize = True\n",
    "    features.append(cid_ce(x, normalize))\n",
    "    \n",
    "    features.append(skewness(x))\n",
    "    features.append(kurtosis(x))\n",
    "    features.append(mean_abs_change(x))\n",
    "    features.append(mean_second_derivative_central(x))\n",
    "    \n",
    "    return np.array(features)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eafc88144037074973094f5d116f381f14498322"
   },
   "source": [
    "### Entropy & Fractial Dimension Features\n",
    "- In addition to the ts-fresh ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def petrosian_fd(x):\n",
    "    \"\"\"Petrosian fractal dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One dimensional time series\n",
    "    Returns\n",
    "    -------\n",
    "    pfd : float\n",
    "        Petrosian fractal dimension\n",
    "    Notes\n",
    "    -----\n",
    "    The Petrosian algorithm can be used to provide a fast computation of\n",
    "    the FD of a signal by translating the series into a binary sequence.\n",
    "    The Petrosian fractal dimension of a time series :math:`x` is defined by:\n",
    "    .. math:: \\dfrac{log_{10}(N)}{log_{10}(N) +\n",
    "       log_{10}(\\dfrac{N}{N+0.4N_{\\Delta}})}\n",
    "    where :math:`N` is the length of the time series, and\n",
    "    :math:`N_{\\Delta}` is the number of sign changes in the binary sequence.\n",
    "    Original code from the pyrem package by Quentin Geissmann.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] A. Petrosian, Kolmogorov complexity of finite sequences and\n",
    "       recognition of different preictal EEG patterns, in , Proceedings of the\n",
    "       Eighth IEEE Symposium on Computer-Based Medical Systems, 1995,\n",
    "       pp. 212-217.\n",
    "    .. [2] Goh, Cindy, et al. \"Comparison of fractal dimension algorithms for\n",
    "       the computation of EEG biomarkers for dementia.\" 2nd International\n",
    "       Conference on Computational Intelligence in Medicine and Healthcare\n",
    "       (CIMED2005). 2005.\n",
    "    Examples\n",
    "    --------\n",
    "    Petrosian fractal dimension.\n",
    "        >>> import numpy as np\n",
    "        >>> from entropy import petrosian_fd\n",
    "        >>> np.random.seed(123)\n",
    "        >>> x = np.random.rand(100)\n",
    "        >>> print(petrosian_fd(x))\n",
    "            1.0505\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    # Number of sign changes in the first derivative of the signal\n",
    "    diff = np.ediff1d(x)\n",
    "    N_delta = (diff[1:-1] * diff[0:-2] < 0).sum()\n",
    "    return np.log10(n) / (np.log10(n) + np.log10(n / (n + 0.4 * N_delta)))\n",
    "\n",
    "\n",
    "def katz_fd(x):\n",
    "    \"\"\"Katz Fractal Dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One dimensional time series\n",
    "    Returns\n",
    "    -------\n",
    "    kfd : float\n",
    "        Katz fractal dimension\n",
    "    Notes\n",
    "    -----\n",
    "    The Katz Fractal dimension is defined by:\n",
    "    .. math:: FD_{Katz} = \\dfrac{log_{10}(n)}{log_{10}(d/L)+log_{10}(n)}\n",
    "    where :math:`L` is the total length of the time series and :math:`d`\n",
    "    is the Euclidean distance between the first point in the\n",
    "    series and the point that provides the furthest distance\n",
    "    with respect to the first point.\n",
    "    Original code from the mne-features package by Jean-Baptiste Schiratti\n",
    "    and Alexandre Gramfort.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Esteller, R. et al. (2001). A comparison of waveform fractal\n",
    "           dimension algorithms. IEEE Transactions on Circuits and Systems I:\n",
    "           Fundamental Theory and Applications, 48(2), 177-183.\n",
    "    .. [2] Goh, Cindy, et al. \"Comparison of fractal dimension algorithms for\n",
    "           the computation of EEG biomarkers for dementia.\" 2nd International\n",
    "           Conference on Computational Intelligence in Medicine and Healthcare\n",
    "           (CIMED2005). 2005.\n",
    "    Examples\n",
    "    --------\n",
    "    Katz fractal dimension.\n",
    "        >>> import numpy as np\n",
    "        >>> from entropy import katz_fd\n",
    "        >>> np.random.seed(123)\n",
    "        >>> x = np.random.rand(100)\n",
    "        >>> print(katz_fd(x))\n",
    "            5.1214\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    dists = np.abs(np.ediff1d(x))\n",
    "    ll = dists.sum()\n",
    "    ln = np.log10(np.divide(ll, dists.mean()))\n",
    "    aux_d = x - x[0]\n",
    "    d = np.max(np.abs(aux_d[1:]))\n",
    "    return np.divide(ln, np.add(ln, np.log10(np.divide(d, ll))))\n",
    "\n",
    "@jit('UniTuple(float64, 2)(float64[:], float64[:])', nopython=True)\n",
    "def _linear_regression(x, y):\n",
    "    \"\"\"Fast linear regression using Numba.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : ndarray, shape (n_times,)\n",
    "        Variables\n",
    "    Returns\n",
    "    -------\n",
    "    slope : float\n",
    "        Slope of 1D least-square regression.\n",
    "    intercept : float\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    n_times = x.size\n",
    "    sx2 = 0\n",
    "    sx = 0\n",
    "    sy = 0\n",
    "    sxy = 0\n",
    "    for j in range(n_times):\n",
    "        sx2 += x[j] ** 2\n",
    "        sx += x[j]\n",
    "        sxy += x[j] * y[j]\n",
    "        sy += y[j]\n",
    "    den = n_times * sx2 - (sx ** 2)\n",
    "    num = n_times * sxy - sx * sy\n",
    "    slope = num / den\n",
    "    intercept = np.mean(y) - slope * np.mean(x)\n",
    "    return slope, intercept\n",
    "\n",
    "\n",
    "@jit('i8[:](f8, f8, f8)', nopython=True)\n",
    "def _log_n(min_n, max_n, factor):\n",
    "    \"\"\"\n",
    "    Creates a list of integer values by successively multiplying a minimum\n",
    "    value min_n by a factor > 1 until a maximum value max_n is reached.\n",
    "    Used for detrended fluctuation analysis (DFA).\n",
    "    Function taken from the nolds python package\n",
    "    (https://github.com/CSchoel/nolds) by Christopher Scholzel.\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_n (float):\n",
    "        minimum value (must be < max_n)\n",
    "    max_n (float):\n",
    "        maximum value (must be > min_n)\n",
    "    factor (float):\n",
    "       factor used to increase min_n (must be > 1)\n",
    "    Returns\n",
    "    -------\n",
    "    list of integers:\n",
    "        min_n, min_n * factor, min_n * factor^2, ... min_n * factor^i < max_n\n",
    "        without duplicates\n",
    "    \"\"\"\n",
    "    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n",
    "    ns = [min_n]\n",
    "    for i in range(max_i + 1):\n",
    "        n = int(floor(min_n * (factor ** i)))\n",
    "        if n > ns[-1]:\n",
    "            ns.append(n)\n",
    "    return np.array(ns, dtype=np.int64)\n",
    "\n",
    "@jit('float64(float64[:], int32)')\n",
    "def _higuchi_fd(x, kmax):\n",
    "    \"\"\"Utility function for `higuchi_fd`.\n",
    "    \"\"\"\n",
    "    n_times = x.size\n",
    "    lk = np.empty(kmax)\n",
    "    x_reg = np.empty(kmax)\n",
    "    y_reg = np.empty(kmax)\n",
    "    for k in range(1, kmax + 1):\n",
    "        lm = np.empty((k,))\n",
    "        for m in range(k):\n",
    "            ll = 0\n",
    "            n_max = floor((n_times - m - 1) / k)\n",
    "            n_max = int(n_max)\n",
    "            for j in range(1, n_max):\n",
    "                ll += abs(x[m + j * k] - x[m + (j - 1) * k])\n",
    "            ll /= k\n",
    "            ll *= (n_times - 1) / (k * n_max)\n",
    "            lm[m] = ll\n",
    "        # Mean of lm\n",
    "        m_lm = 0\n",
    "        for m in range(k):\n",
    "            m_lm += lm[m]\n",
    "        m_lm /= k\n",
    "        lk[k - 1] = m_lm\n",
    "        x_reg[k - 1] = log(1. / k)\n",
    "        y_reg[k - 1] = log(m_lm)\n",
    "    higuchi, _ = _linear_regression(x_reg, y_reg)\n",
    "    return higuchi\n",
    "\n",
    "\n",
    "def higuchi_fd(x, kmax=10):\n",
    "    \"\"\"Higuchi Fractal Dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One dimensional time series\n",
    "    kmax : int\n",
    "        Maximum delay/offset (in number of samples).\n",
    "    Returns\n",
    "    -------\n",
    "    hfd : float\n",
    "        Higuchi Fractal Dimension\n",
    "    Notes\n",
    "    -----\n",
    "    Original code from the mne-features package by Jean-Baptiste Schiratti\n",
    "    and Alexandre Gramfort.\n",
    "    The `higuchi_fd` function uses Numba to speed up the computation.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Higuchi, Tomoyuki. \"Approach to an irregular time series on the\n",
    "       basis of the fractal theory.\" Physica D: Nonlinear Phenomena 31.2\n",
    "       (1988): 277-283.\n",
    "    Examples\n",
    "    --------\n",
    "    Higuchi Fractal Dimension\n",
    "        >>> import numpy as np\n",
    "        >>> from entropy import higuchi_fd\n",
    "        >>> np.random.seed(123)\n",
    "        >>> x = np.random.rand(100)\n",
    "        >>> print(higuchi_fd(x))\n",
    "            2.051179\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    kmax = int(kmax)\n",
    "    return _higuchi_fd(x, kmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def _embed(x, order=3, delay=1):\n",
    "    \"\"\"Time-delay embedding.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 1d-array, shape (n_times)\n",
    "        Time series\n",
    "    order : int\n",
    "        Embedding dimension (order)\n",
    "    delay : int\n",
    "        Delay.\n",
    "    Returns\n",
    "    -------\n",
    "    embedded : ndarray, shape (n_times - (order - 1) * delay, order)\n",
    "        Embedded time-series.\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    if order * delay > N:\n",
    "        raise ValueError(\"Error: order * delay should be lower than x.size\")\n",
    "    if delay < 1:\n",
    "        raise ValueError(\"Delay has to be at least 1.\")\n",
    "    if order < 2:\n",
    "        raise ValueError(\"Order has to be at least 2.\")\n",
    "    Y = np.zeros((order, N - (order - 1) * delay))\n",
    "    for i in range(order):\n",
    "        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n",
    "    return Y.T\n",
    "\n",
    "all = ['perm_entropy', 'spectral_entropy', 'svd_entropy', 'app_entropy',\n",
    "       'sample_entropy']\n",
    "\n",
    "\n",
    "def perm_entropy(x, order=3, delay=1, normalize=False):\n",
    "    \"\"\"Permutation Entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One-dimensional time series of shape (n_times)\n",
    "    order : int\n",
    "        Order of permutation entropy\n",
    "    delay : int\n",
    "        Time delay\n",
    "    normalize : bool\n",
    "        If True, divide by log2(order!) to normalize the entropy between 0\n",
    "        and 1. Otherwise, return the permutation entropy in bit.\n",
    "    Returns\n",
    "    -------\n",
    "    pe : float\n",
    "        Permutation Entropy\n",
    "    Notes\n",
    "    -----\n",
    "    The permutation entropy is a complexity measure for time-series first\n",
    "    introduced by Bandt and Pompe in 2002 [1]_.\n",
    "    The permutation entropy of a signal :math:`x` is defined as:\n",
    "    .. math:: H = -\\sum p(\\pi)log_2(\\pi)\n",
    "    where the sum runs over all :math:`n!` permutations :math:`\\pi` of order\n",
    "    :math:`n`. This is the information contained in comparing :math:`n`\n",
    "    consecutive values of the time series. It is clear that\n",
    "    :math:`0 ≤ H (n) ≤ log_2(n!)` where the lower bound is attained for an\n",
    "    increasing or decreasing sequence of values, and the upper bound for a\n",
    "    completely random system where all :math:`n!` possible permutations appear\n",
    "    with the same probability.\n",
    "    The embedded matrix :math:`Y` is created by:\n",
    "    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]\n",
    "    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a\n",
    "           natural complexity measure for time series.\" Physical review letters\n",
    "           88.17 (2002): 174102.\n",
    "    Examples\n",
    "    --------\n",
    "    1. Permutation entropy with order 2\n",
    "        >>> from entropy import perm_entropy\n",
    "        >>> x = [4, 7, 9, 10, 6, 11, 3]\n",
    "        >>> # Return a value in bit between 0 and log2(factorial(order))\n",
    "        >>> print(perm_entropy(x, order=2))\n",
    "            0.918\n",
    "    2. Normalized permutation entropy with order 3\n",
    "        >>> from entropy import perm_entropy\n",
    "        >>> x = [4, 7, 9, 10, 6, 11, 3]\n",
    "        >>> # Return a value comprised between 0 and 1.\n",
    "        >>> print(perm_entropy(x, order=3, normalize=True))\n",
    "            0.589\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    ran_order = range(order)\n",
    "    hashmult = np.power(order, ran_order)\n",
    "    # Embed x and sort the order of permutations\n",
    "    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n",
    "    # Associate unique integer to each permutations\n",
    "    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n",
    "    # Return the counts\n",
    "    _, c = np.unique(hashval, return_counts=True)\n",
    "    # Use np.true_divide for Python 2 compatibility\n",
    "    p = np.true_divide(c, c.sum())\n",
    "    pe = -np.multiply(p, np.log2(p)).sum()\n",
    "    if normalize:\n",
    "        pe /= np.log2(factorial(order))\n",
    "    return pe\n",
    "\n",
    "\n",
    "def spectral_entropy(x, sf, method='fft', nperseg=None, normalize=False):\n",
    "    \"\"\"Spectral Entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One-dimensional time series of shape (n_times)\n",
    "    sf : float\n",
    "        Sampling frequency\n",
    "    method : str\n",
    "        Spectral estimation method ::\n",
    "        'fft' : Fourier Transform (via scipy.signal.periodogram)\n",
    "        'welch' : Welch periodogram (via scipy.signal.welch)\n",
    "    nperseg : str or int\n",
    "        Length of each FFT segment for Welch method.\n",
    "        If None, uses scipy default of 256 samples.\n",
    "    normalize : bool\n",
    "        If True, divide by log2(psd.size) to normalize the spectral entropy\n",
    "        between 0 and 1. Otherwise, return the spectral entropy in bit.\n",
    "    Returns\n",
    "    -------\n",
    "    se : float\n",
    "        Spectral Entropy\n",
    "    Notes\n",
    "    -----\n",
    "    Spectral Entropy is defined to be the Shannon Entropy of the Power\n",
    "    Spectral Density (PSD) of the data:\n",
    "    .. math:: H(x, sf) =  -\\sum_{f=0}^{f_s/2} PSD(f) log_2[PSD(f)]\n",
    "    Where :math:`PSD` is the normalised PSD, and :math:`f_s` is the sampling\n",
    "    frequency.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Inouye, T. et al. (1991). Quantification of EEG irregularity by\n",
    "       use of the entropy of the power spectrum. Electroencephalography\n",
    "       and clinical neurophysiology, 79(3), 204-210.\n",
    "    Examples\n",
    "    --------\n",
    "    1. Spectral entropy of a pure sine using FFT\n",
    "        >>> from entropy import spectral_entropy\n",
    "        >>> import numpy as np\n",
    "        >>> sf, f, dur = 100, 1, 4\n",
    "        >>> N = sf * duration # Total number of discrete samples\n",
    "        >>> t = np.arange(N) / sf # Time vector\n",
    "        >>> x = np.sin(2 * np.pi * f * t)\n",
    "        >>> print(np.round(spectral_entropy(x, sf, method='fft'), 2)\n",
    "            0.0\n",
    "    2. Spectral entropy of a random signal using Welch's method\n",
    "        >>> from entropy import spectral_entropy\n",
    "        >>> import numpy as np\n",
    "        >>> np.random.seed(42)\n",
    "        >>> x = np.random.rand(3000)\n",
    "        >>> print(spectral_entropy(x, sf=100, method='welch'))\n",
    "            9.939\n",
    "    3. Normalized spectral entropy\n",
    "        >>> print(spectral_entropy(x, sf=100, method='welch', normalize=True))\n",
    "            0.995\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    # Compute and normalize power spectrum\n",
    "    if method == 'fft':\n",
    "        _, psd = periodogram(x, sf)\n",
    "    elif method == 'welch':\n",
    "        _, psd = welch(x, sf, nperseg=nperseg)\n",
    "    psd_norm = np.divide(psd, psd.sum())\n",
    "    se = -np.multiply(psd_norm, np.log2(psd_norm)).sum()\n",
    "    if normalize:\n",
    "        se /= np.log2(psd_norm.size)\n",
    "    return se\n",
    "\n",
    "\n",
    "def svd_entropy(x, order=3, delay=1, normalize=False):\n",
    "    \"\"\"Singular Value Decomposition entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One-dimensional time series of shape (n_times)\n",
    "    order : int\n",
    "        Order of permutation entropy\n",
    "    delay : int\n",
    "        Time delay\n",
    "    normalize : bool\n",
    "        If True, divide by log2(order!) to normalize the entropy between 0\n",
    "        and 1. Otherwise, return the permutation entropy in bit.\n",
    "    Returns\n",
    "    -------\n",
    "    svd_e : float\n",
    "        SVD Entropy\n",
    "    Notes\n",
    "    -----\n",
    "    SVD entropy is an indicator of the number of eigenvectors that are needed\n",
    "    for an adequate explanation of the data set. In other words, it measures\n",
    "    the dimensionality of the data.\n",
    "    The SVD entropy of a signal :math:`x` is defined as:\n",
    "    .. math::\n",
    "        H = -\\sum_{i=1}^{M} \\overline{\\sigma}_i log_2(\\overline{\\sigma}_i)\n",
    "    where :math:`M` is the number of singular values of the embedded matrix\n",
    "    :math:`Y` and :math:`\\sigma_1, \\sigma_2, ..., \\sigma_M` are the\n",
    "    normalized singular values of :math:`Y`.\n",
    "    The embedded matrix :math:`Y` is created by:\n",
    "    .. math:: y(i)=[x_i,x_{i+delay}, ...,x_{i+(order-1) * delay}]\n",
    "    .. math:: Y=[y(1),y(2),...,y(N-(order-1))*delay)]^T\n",
    "    Examples\n",
    "    --------\n",
    "    1. SVD entropy with order 2\n",
    "        >>> from entropy import svd_entropy\n",
    "        >>> x = [4, 7, 9, 10, 6, 11, 3]\n",
    "        >>> # Return a value in bit between 0 and log2(factorial(order))\n",
    "        >>> print(svd_entropy(x, order=2))\n",
    "            0.762\n",
    "    2. Normalized SVD entropy with order 3\n",
    "        >>> from entropy import svd_entropy\n",
    "        >>> x = [4, 7, 9, 10, 6, 11, 3]\n",
    "        >>> # Return a value comprised between 0 and 1.\n",
    "        >>> print(svd_entropy(x, order=3, normalize=True))\n",
    "            0.687\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    mat = _embed(x, order=order, delay=delay)\n",
    "    W = np.linalg.svd(mat, compute_uv=False)\n",
    "    # Normalize the singular values\n",
    "    W /= sum(W)\n",
    "    svd_e = -np.multiply(W, np.log2(W)).sum()\n",
    "    if normalize:\n",
    "        svd_e /= np.log2(order)\n",
    "    return svd_e\n",
    "\n",
    "\n",
    "def _app_samp_entropy(x, order, metric='chebyshev', approximate=True):\n",
    "    \"\"\"Utility function for `app_entropy`` and `sample_entropy`.\n",
    "    \"\"\"\n",
    "    _all_metrics = KDTree.valid_metrics\n",
    "    if metric not in _all_metrics:\n",
    "        raise ValueError('The given metric (%s) is not valid. The valid '\n",
    "                         'metric names are: %s' % (metric, _all_metrics))\n",
    "    phi = np.zeros(2)\n",
    "    r = 0.2 * np.std(x, axis=-1, ddof=1)\n",
    "\n",
    "    # compute phi(order, r)\n",
    "    _emb_data1 = _embed(x, order, 1)\n",
    "    if approximate:\n",
    "        emb_data1 = _emb_data1\n",
    "    else:\n",
    "        emb_data1 = _emb_data1[:-1]\n",
    "    count1 = KDTree(emb_data1, metric=metric).query_radius(emb_data1, r,\n",
    "                                                           count_only=True\n",
    "                                                           ).astype(np.float64)\n",
    "    # compute phi(order + 1, r)\n",
    "    emb_data2 = _embed(x, order + 1, 1)\n",
    "    count2 = KDTree(emb_data2, metric=metric).query_radius(emb_data2, r,\n",
    "                                                           count_only=True\n",
    "                                                           ).astype(np.float64)\n",
    "    if approximate:\n",
    "        phi[0] = np.mean(np.log(count1 / emb_data1.shape[0]))\n",
    "        phi[1] = np.mean(np.log(count2 / emb_data2.shape[0]))\n",
    "    else:\n",
    "        phi[0] = np.mean((count1 - 1) / (emb_data1.shape[0] - 1))\n",
    "        phi[1] = np.mean((count2 - 1) / (emb_data2.shape[0] - 1))\n",
    "    return phi\n",
    "\n",
    "\n",
    "@jit('f8(f8[:], i4, f8)', nopython=True)\n",
    "def _numba_sampen(x, mm=2, r=0.2):\n",
    "    \"\"\"\n",
    "    Fast evaluation of the sample entropy using Numba.\n",
    "    \"\"\"\n",
    "    n = x.size\n",
    "    n1 = n - 1\n",
    "    mm += 1\n",
    "    mm_dbld = 2 * mm\n",
    "\n",
    "    # Define threshold\n",
    "    r *= x.std()\n",
    "\n",
    "    # initialize the lists\n",
    "    run = [0] * n\n",
    "    run1 = run[:]\n",
    "    r1 = [0] * (n * mm_dbld)\n",
    "    a = [0] * mm\n",
    "    b = a[:]\n",
    "    p = a[:]\n",
    "\n",
    "    for i in range(n1):\n",
    "        nj = n1 - i\n",
    "\n",
    "        for jj in range(nj):\n",
    "            j = jj + i + 1\n",
    "            if abs(x[j] - x[i]) < r:\n",
    "                run[jj] = run1[jj] + 1\n",
    "                m1 = mm if mm < run[jj] else run[jj]\n",
    "                for m in range(m1):\n",
    "                    a[m] += 1\n",
    "                    if j < n1:\n",
    "                        b[m] += 1\n",
    "            else:\n",
    "                run[jj] = 0\n",
    "        for j in range(mm_dbld):\n",
    "            run1[j] = run[j]\n",
    "            r1[i + n * j] = run[j]\n",
    "        if nj > mm_dbld - 1:\n",
    "            for j in range(mm_dbld, nj):\n",
    "                run1[j] = run[j]\n",
    "\n",
    "    m = mm - 1\n",
    "\n",
    "    while m > 0:\n",
    "        b[m] = b[m - 1]\n",
    "        m -= 1\n",
    "\n",
    "    b[0] = n * n1 / 2\n",
    "    a = np.array([float(aa) for aa in a])\n",
    "    b = np.array([float(bb) for bb in b])\n",
    "    p = np.true_divide(a, b)\n",
    "    return -log(p[-1])\n",
    "\n",
    "\n",
    "def app_entropy(x, order=2, metric='chebyshev'):\n",
    "    \"\"\"Approximate Entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One-dimensional time series of shape (n_times)\n",
    "    order : int (default: 2)\n",
    "        Embedding dimension.\n",
    "    metric : str (default: chebyshev)\n",
    "        Name of the metric function used with\n",
    "        :class:`~sklearn.neighbors.KDTree`. The list of available\n",
    "        metric functions is given by: ``KDTree.valid_metrics``.\n",
    "    Returns\n",
    "    -------\n",
    "    ae : float\n",
    "        Approximate Entropy.\n",
    "    Notes\n",
    "    -----\n",
    "    Original code from the mne-features package.\n",
    "    Approximate entropy is a technique used to quantify the amount of\n",
    "    regularity and the unpredictability of fluctuations over time-series data.\n",
    "    Smaller values indicates that the data is more regular and predictable.\n",
    "    The value of :math:`r` is set to :math:`0.2 * std(x)`.\n",
    "    Code adapted from the mne-features package by Jean-Baptiste Schiratti\n",
    "    and Alexandre Gramfort.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
    "           using approximate entropy and sample entropy. American Journal of\n",
    "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
    "    1. Approximate entropy with order 2.\n",
    "        >>> from entropy import app_entropy\n",
    "        >>> import numpy as np\n",
    "        >>> np.random.seed(1234567)\n",
    "        >>> x = np.random.rand(3000)\n",
    "        >>> print(app_entropy(x, order=2))\n",
    "            2.075\n",
    "    \"\"\"\n",
    "    phi = _app_samp_entropy(x, order=order, metric=metric, approximate=True)\n",
    "    return np.subtract(phi[0], phi[1])\n",
    "\n",
    "\n",
    "def sample_entropy(x, order=2, metric='chebyshev'):\n",
    "    \"\"\"Sample Entropy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list or np.array\n",
    "        One-dimensional time series of shape (n_times)\n",
    "    order : int (default: 2)\n",
    "        Embedding dimension.\n",
    "    metric : str (default: chebyshev)\n",
    "        Name of the metric function used with KDTree. The list of available\n",
    "        metric functions is given by: `KDTree.valid_metrics`.\n",
    "    Returns\n",
    "    -------\n",
    "    se : float\n",
    "        Sample Entropy.\n",
    "    Notes\n",
    "    -----\n",
    "    Sample entropy is a modification of approximate entropy, used for assessing\n",
    "    the complexity of physiological time-series signals. It has two advantages\n",
    "    over approximate entropy: data length independence and a relatively\n",
    "    trouble-free implementation. Large values indicate high complexity whereas\n",
    "    smaller values characterize more self-similar and regular signals.\n",
    "    Sample entropy of a signal :math:`x` is defined as:\n",
    "    .. math:: H(x, m, r) = -log\\dfrac{C(m + 1, r)}{C(m, r)}\n",
    "    where :math:`m` is the embedding dimension (= order), :math:`r` is\n",
    "    the radius of the neighbourhood (default = :math:`0.2 * std(x)`),\n",
    "    :math:`C(m + 1, r)` is the number of embedded vectors of length\n",
    "    :math:`m + 1` having a Chebyshev distance inferior to :math:`r` and\n",
    "    :math:`C(m, r)` is the number of embedded vectors of length\n",
    "    :math:`m` having a Chebyshev distance inferior to :math:`r`.\n",
    "    Note that if metric == 'chebyshev' and x.size < 5000 points, then the\n",
    "    sample entropy is computed using a fast custom Numba script. For other\n",
    "    metric types or longer time-series, the sample entropy is computed using\n",
    "    a code from the mne-features package by Jean-Baptiste Schiratti\n",
    "    and Alexandre Gramfort (requires sklearn).\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Richman, J. S. et al. (2000). Physiological time-series analysis\n",
    "           using approximate entropy and sample entropy. American Journal of\n",
    "           Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
    "    Examples\n",
    "    --------\n",
    "    1. Sample entropy with order 2.\n",
    "        >>> from entropy import sample_entropy\n",
    "        >>> import numpy as np\n",
    "        >>> np.random.seed(1234567)\n",
    "        >>> x = np.random.rand(3000)\n",
    "        >>> print(sample_entropy(x, order=2))\n",
    "            2.192\n",
    "    2. Sample entropy with order 3 using the Euclidean distance.\n",
    "        >>> from entropy import sample_entropy\n",
    "        >>> import numpy as np\n",
    "        >>> np.random.seed(1234567)\n",
    "        >>> x = np.random.rand(3000)\n",
    "        >>> print(sample_entropy(x, order=3, metric='euclidean'))\n",
    "            2.725\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    if metric == 'chebyshev' and x.size < 5000:\n",
    "        return _numba_sampen(x, mm=order, r=0.2)\n",
    "    else:\n",
    "        phi = _app_samp_entropy(x, order=order, metric=metric,\n",
    "                                approximate=False)\n",
    "        return -np.log(np.divide(phi[1], phi[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def entropy_and_fractal_dim(x):\n",
    "    return np.array([perm_entropy(x), svd_entropy(x), petrosian_fd(x), katz_fd(x), higuchi_fd(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c67064aa4abeb5ec26b99cd7fc49f01e2b174db"
   },
   "source": [
    "## Bucketting\n",
    "- Features are computed on segments of the signal of size *bucket_size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(signal, bucket_size=5000):\n",
    "    return signal.reshape((signal.shape[0]//bucket_size, bucket_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_bucket(signal, function):\n",
    "    return np.array([function(s) for s in signal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c67064aa4abeb5ec26b99cd7fc49f01e2b174db"
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "39502a1c645f309eeb5a8e3ba99b34b954c9d327"
   },
   "outputs": [],
   "source": [
    "def generate_peak_features(signal):\n",
    "    signal = wavelet_lp(signal)\n",
    "    signal = bucket(signal)\n",
    "    features = features_bucket(signal, peak_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "39502a1c645f309eeb5a8e3ba99b34b954c9d327"
   },
   "outputs": [],
   "source": [
    "def generate_dist_features(signal):\n",
    "    signal = wavelet_lp(signal)\n",
    "    signal = bucket(signal)\n",
    "    features = features_bucket(signal, dist_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "39502a1c645f309eeb5a8e3ba99b34b954c9d327"
   },
   "outputs": [],
   "source": [
    "def generate_tsfresh_features(signal):\n",
    "    signal = wavelet_lp(signal)\n",
    "    signal = bucket(signal)\n",
    "    return features_bucket(signal, tsfresh_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "39502a1c645f309eeb5a8e3ba99b34b954c9d327"
   },
   "outputs": [],
   "source": [
    "def generate_entfrac_features(signal):\n",
    "    signal = wavelet_lp(signal)\n",
    "    return entropy_and_fractal_dim(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = \"train\"\n",
    "generate = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"final\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c67064aa4abeb5ec26b99cd7fc49f01e2b174db"
   },
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if generate == \"train\":\n",
    "    signals = pq.read_table(TRAIN_PATH).to_pandas()\n",
    "    signals = np.array(signals).T\n",
    "    \n",
    "    X = []\n",
    "    for i in tqdm(range(len(signals))):\n",
    "        X.append(generate_dist_features(signals[i]))\n",
    "    X = np.array(X)\n",
    "    \n",
    "    np.save(f\"{OUT_PATH}/X_{name}.npy\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c67064aa4abeb5ec26b99cd7fc49f01e2b174db"
   },
   "source": [
    "### Test Data\n",
    "Test data is too big, we process it in 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "98e27585de501810aa5e0c71dfee0b70ef8724a9"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/data/sample_submission.csv')\n",
    "divs = list(range(sub[\"signal_id\"].values[0], sub[\"signal_id\"].values[-1], 8000))  + [sub[\"signal_id\"].values[-1] + 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0123cc4c2d98c85366c7add038d8d7782269dfc",
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f3cbddc8b94c87a4e7867a68804fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 800000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4994f802f6e54b678db9fd55bc414d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 800000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d03a9b58fb47628228da04c075e827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "if generate == \"test\":\n",
    "    for i in tqdm(range(1, len(divs))):\n",
    "        signals = pq.read_table(TEST_PATH, columns=[str(j) for j in range(divs[i-1], divs[i])]).to_pandas()\n",
    "        signals = np.array(signals).T\n",
    "        print(signals.shape)\n",
    "\n",
    "        X_test = []\n",
    "        for j in tqdm(range(len(signals))):\n",
    "            X_test.append(generate_tsfresh_features(signals[j]))\n",
    "        X_test = np.array(X_test)\n",
    "\n",
    "        np.save(f\"{OUT_PATH}/X_test_{name}_{i}.npy\", X_test)\n",
    "        del signals, X_test\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
